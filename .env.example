# ╔══════════════════════════════════════════════════════════════════════════╗
# ║  nulpoint Gateway — Environment Configuration                            ║
# ║                                                                          ║
# ║  Copy this file to .env and fill in your values.                         ║
# ║  All variables can also be passed as real env vars (env vars win).       ║
# ║                                                                          ║
# ║  At least ONE provider key is required. The gateway enables only         ║
# ║  the providers whose keys are set — all others are silently skipped.     ║
# ╚══════════════════════════════════════════════════════════════════════════╝

# ── Tier-1 Providers ─────────────────────────────────────────────────────────
# Large proprietary models with native SDKs.

OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AIza...          # Google AI Studio (Gemini)
MISTRAL_API_KEY=...

# ── xAI (Grok) ───────────────────────────────────────────────────────────────
# Models: grok-3, grok-3-fast, grok-3-mini, grok-3-mini-fast, grok-2-1212,
#         grok-2-vision-1212, grok-beta, grok-vision-beta...
# XAI_API_KEY=xai-...

# ── DeepSeek ─────────────────────────────────────────────────────────────────
# Models: deepseek-chat, deepseek-reasoner...
# DEEPSEEK_API_KEY=sk-...

# ── Groq ─────────────────────────────────────────────────────────────────────
# Models: llama-3.3-70b-versatile, llama-3.1-70b-versatile,
#         llama-3.1-8b-instant, llama3-70b-8192, llama3-8b-8192, gemma2-9b-it...
# GROQ_API_KEY=gsk_...

# ── Together AI ──────────────────────────────────────────────────────────────
# Models: meta-llama/Llama-3.3-70B-Instruct-Turbo,
#         meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo,
#         mistralai/Mixtral-8x7B-Instruct-v0.1,
#         Qwen/Qwen2.5-72B-Instruct-Turbo, deepseek-ai/DeepSeek-R1, …
# TOGETHER_API_KEY=...

# ── Perplexity ───────────────────────────────────────────────────────────────
# Models: sonar, sonar-pro, sonar-reasoning (real-time web access)
# PERPLEXITY_API_KEY=pplx-...

# ── Cerebras ─────────────────────────────────────────────────────────────────
# Models: llama3.1-8b, llama3.1-70b, llama3.3-70b,
#         qwen-3-32b, qwen-3-235b, deepseek-r1-distill-llama-70b
# CEREBRAS_API_KEY=csk-...

# ── Moonshot AI ──────────────────────────────────────────────────────────────
# Models: moonshot-v1-8k, moonshot-v1-32k, moonshot-v1-128k,
#         moonshot-v1-auto, kimi-latest
# MOONSHOT_API_KEY=...

# ── MiniMax ──────────────────────────────────────────────────────────────────
# Models: MiniMax-Text-01, MiniMax-VL-01, abab6.5s-chat, abab6.5-chat
# MINIMAX_API_KEY=...

# ── Alibaba Cloud (Qwen) ─────────────────────────────────────────────────────
# Models: qwen-turbo, qwen-plus, qwen-max, qwq-plus, qwq-32b,
#         qwen2.5-72b-instruct, qwen-vl-max, …
# QWEN_API_KEY=sk-...

# ── Nebius AI Studio ─────────────────────────────────────────────────────────
# Models: meta-llama/Meta-Llama-3.1-70B-Instruct,
#         meta-llama/Meta-Llama-3.3-70B-Instruct,
#         Qwen/Qwen2.5-72B-Instruct, deepseek-ai/DeepSeek-V3, …
# NEBIUS_API_KEY=...

# ── NovitaAI ─────────────────────────────────────────────────────────────────
# Models: meta-llama/llama-3.1-8b-instruct,
#         meta-llama/llama-3.1-70b-instruct,
#         deepseek/deepseek-v3, deepseek/deepseek-r1, …
# NOVITA_API_KEY=...

# ── ByteDance ModelArk ───────────────────────────────────────────────────────
# Models: doubao-1.5-pro-32k, doubao-1.5-lite-32k,
#         doubao-pro-32k, doubao-pro-128k, …
# BYTEDANCE_API_KEY=...

# ── Z AI ─────────────────────────────────────────────────────────────────────
# Models: glm-4-plus, glm-4-air, glm-4-flash, glm-4, glm-3-turbo, …
# ZAI_API_KEY=...

# ── CanopyWave ───────────────────────────────────────────────────────────────
# OpenAI-compatible inference infrastructure provider.
# CANOPYWAVE_API_KEY=...

# ── Inference.net ────────────────────────────────────────────────────────────
# Models: inference-llama-3.1-8b, inference-llama-3.1-70b
# INFERENCE_API_KEY=...

# ── NanoGPT ──────────────────────────────────────────────────────────────────
# Multi-model aggregator with OpenAI-compatible API.
# NANOGPT_API_KEY=...

# ── Google Vertex AI ─────────────────────────────────────────────────────────
# Same Gemini models as Google AI Studio but via Google Cloud with ADC auth.
# Auth: set GOOGLE_APPLICATION_CREDENTIALS to a service account key file,
#       or run on GCE/GKE where the metadata server provides credentials.
# Use "vertexai-" prefix in model name for explicit routing:
#   e.g. model="vertexai-gemini-2.0-flash"
# VERTEX_PROJECT=my-gcp-project
# VERTEX_LOCATION=us-central1

# ── AWS Bedrock ──────────────────────────────────────────────────────────────
# Bedrock model IDs use the provider-namespaced format:
#   anthropic.claude-3-5-sonnet-20241022-v2:0
#   meta.llama3-70b-instruct-v1:0
#   amazon.nova-pro-v1:0
# AWS_ACCESS_KEY_ID=AKIA...
# AWS_SECRET_ACCESS_KEY=...
# AWS_SESSION_TOKEN=...        # optional — for temporary STS credentials
# AWS_REGION=us-east-1

# ── Azure OpenAI ─────────────────────────────────────────────────────────────
# Azure uses deployment-based URLs. Strip the "azure-" prefix from the model
# name to get the deployment name:  model="azure-gpt-4o" → deployment "gpt-4o"
# AZURE_OPENAI_ENDPOINT=https://myresource.openai.azure.com
# AZURE_OPENAI_API_KEY=...
# AZURE_OPENAI_API_VERSION=2024-12-01-preview

# ── Server ───────────────────────────────────────────────────────────────────

PORT=8080

# Log level: debug | info | warn | error  (default: info)
# JSON structured logs are written to stdout.
LOG_LEVEL=info

# Allow clients to send Authorization: Bearer tokens that are forwarded directly
# to the upstream provider. When false, only the keys configured above are used.
# ALLOW_CLIENT_API_KEYS=false

# ── Cache ────────────────────────────────────────────────────────────────────
# CACHE_MODE controls the cache backend:
#   memory  — built-in in-process cache, no external deps (default)
#   redis   — Redis-backed cache, shared across all replicas (production)
#   none    — cache disabled entirely

CACHE_MODE=memory

# TTL for cached responses (Go duration string). Default: 1h
# CACHE_TTL=1h

# Redis connection — required only when CACHE_MODE=redis
# REDIS_URL=redis://localhost:6379

# Models that should never be cached (comma-separated exact names).
# CACHE_EXCLUDE_EXACT=grok-3,sonar-reasoning

# Models that should never be cached (comma-separated Go regexes).
# CACHE_EXCLUDE_PATTERNS=.*-realtime.*,.*-search.*

# ── Circuit Breaker ──────────────────────────────────────────────────────────
# Consecutive failures that trip the breaker (default: 5)
# CB_ERROR_THRESHOLD=5

# Rolling window for counting failures (default: 60s)
# CB_TIME_WINDOW=60s

# How long the breaker stays open before trying a probe (default: 30s)
# CB_HALF_OPEN_TIMEOUT=30s

# ── Failover ─────────────────────────────────────────────────────────────────
# Max provider attempts per request, including the first (default: 3)
# MAX_RETRIES=3

# Per-provider HTTP timeout (default: 30s)
# PROVIDER_TIMEOUT=30s

# ── Rate Limiting ─────────────────────────────────────────────────────────────
# Global requests-per-minute limit. 0 = disabled. Requires CACHE_MODE=redis.
# RPM_LIMIT=0

# ── CORS ─────────────────────────────────────────────────────────────────────
# Comma-separated allowed origins. Default: * (allow all)
# CORS_ORIGINS=https://app.example.com,https://dashboard.example.com

# ── Application ──────────────────────────────────────────────────────────────
# Base URL used in webhook callbacks etc.
# APP_BASE_URL=https://gateway.example.com
